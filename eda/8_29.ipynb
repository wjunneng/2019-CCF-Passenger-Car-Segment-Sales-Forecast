{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "project_path = '/home/wjunneng/Ubuntu/2019-CCF-Passenger-Car-Segment-Sales-Forecast'\n",
    "\n",
    "# train_sales_data\n",
    "train_sales_data_path = project_path + '/data/original/train_sales_data.csv'\n",
    "# train_search_data\n",
    "train_search_data_path = project_path + '/data/original/train_search_data.csv'\n",
    "# train_user_reply_data\n",
    "train_user_reply_data = project_path + '/data/original/train_user_reply_data.csv'\n",
    "\n",
    "# evaluation_public\n",
    "evaluation_public = project_path + '/data/original/evaluation_public.csv'\n",
    "# submit_example.csv\n",
    "submit_example = project_path + '/data/original/submit_example.csv'\n",
    "\n",
    "train_sales_data = pd.read_csv(train_sales_data_path)\n",
    "train_search_data = pd.read_csv(train_search_data_path)\n",
    "train_user_reply_data = pd.read_csv(train_user_reply_data)\n",
    "evaluation_public = pd.read_csv(evaluation_public)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Index(['province', 'adcode', 'model', 'bodyType', 'regYear', 'regMonth',\n       'salesVolume', 'popularity'],\n      dtype='object')\nIndex(['id', 'province', 'adcode', 'model', 'regYear', 'regMonth',\n       'forecastVolum'],\n      dtype='object')\n(36960, 5)\n   adcode             model  regMonth  regYear  salesVolume\n0  310000  3c974920a76ac9c1         1     2016        292.0\n1  530000  3c974920a76ac9c1         1     2016        466.0\n2  150000  3c974920a76ac9c1         1     2016        257.0\n3  110000  3c974920a76ac9c1         1     2016        408.0\n4  510000  3c974920a76ac9c1         1     2016        610.0\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/home/wjunneng/Python/anaconda3/envs/lightgbm/lib/python3.6/site-packages/ipykernel_launcher.py:20: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\nof pandas will change to not sort by default.\n\nTo accept the future behavior, pass 'sort=False'.\n\nTo retain the current behavior and silence the warning, pass 'sort=True'.\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 合并了sales + search\n",
    "train_sales_search = pd.merge(train_sales_data, train_search_data, on=['province','adcode','model','regYear', 'regMonth'])\n",
    "# 合并了sales + search + user_reply\n",
    "train_sales_search_user_reply = pd.merge(train_sales_search, train_user_reply_data, on=['model', 'regYear', 'regMonth'])\n",
    "\n",
    "print(train_sales_search.columns)\n",
    "print(evaluation_public.columns)\n",
    "\n",
    "del evaluation_public['forecastVolum']\n",
    "del evaluation_public['province']\n",
    "del train_sales_search['province']\n",
    "del train_sales_search['bodyType']\n",
    "del train_sales_search['popularity']\n",
    "\n",
    "X_test_id = evaluation_public['id']\n",
    "del evaluation_public['id']\n",
    "\n",
    "X = pd.concat([train_sales_search, evaluation_public], axis=0)\n",
    "print(X.shape)\n",
    "print(X.head())\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "   adcode  model  regMonth  regYear\n0      31     13         1     2018\n1      53     13         1     2018\n2      15     13         1     2018\n3      11     13         1     2018\n4      51     13         1     2018\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 删除province\n",
    "\n",
    "X['adcode'] = X['adcode'].apply(lambda x: int(str(x)[:2]))\n",
    "X['model'] = LabelEncoder().fit_transform(X['model'])\n",
    "\n",
    "X_test = X.iloc[train_sales_search.shape[0]:, :]\n",
    "X = X.iloc[:train_sales_search.shape[0], :]\n",
    "\n",
    "del X_test['salesVolume']\n",
    "print(X_test.head())\n",
    "\n",
    "columns = list(X.columns)\n",
    "columns.remove('salesVolume')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = X[columns], np.log1p(X['salesVolume'])\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=23)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def rmspe(y, yhat):\n",
    "    return np.sqrt(np.mean((yhat/y-1) ** 2))\n",
    "\n",
    "#xgboost\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y,yhat)\n",
    "\n",
    "#lightgbm\n",
    "def rmspe_lgb(yhat, y):\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y,yhat), False\n",
    "\n",
    "#sklearn interface\n",
    "def rmspe_gscv(y, yhat):\n",
    "    y = np.expm1(y)\n",
    "    yhat = np.expm1(yhat)\n",
    "    return rmspe(y, yhat)\n",
    "\n",
    "rmspe_score = make_scorer(rmspe_gscv)\n",
    "kfold = KFold(n_splits=5, random_state=23)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def xgb_model(X_train, X_valid, y_train, y_valid):\n",
    "    \"\"\"\n",
    "    xgb 模型\n",
    "    :param X_train:\n",
    "    :param X_valid:\n",
    "    :param y_train:\n",
    "    :param y_valid:\n",
    "    :param X_test_id:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import xgboost as xgb\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train.values)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid.values)\n",
    "\n",
    "    # ########################################## Tuning Paramters ##########################################\n",
    "    xgb_best_params = {}\n",
    "    params = {'booster': 'gbtree',\n",
    "              'objective': 'reg:squarederror',\n",
    "              'max_depth': 6,\n",
    "              'learning_rate': 1,\n",
    "              'gamma': 0,\n",
    "              'min_child_weight': 1,\n",
    "              'subsample': 1,\n",
    "              'colsample_bytree': 1,\n",
    "              'reg_alpha': 0,\n",
    "              'reg_lambda ': 1,\n",
    "              'random_state': 23,\n",
    "              'gpu_id': 0,\n",
    "              'max_bin': 16,\n",
    "              'tree_method': 'gpu_exact'\n",
    "              }\n",
    "\n",
    "    # ########################################### n_estimators  ############################################\n",
    "    min_merror = np.inf\n",
    "    for n_estimators in range(10, 100, 10):\n",
    "        params['n_estimators'] = n_estimators\n",
    "        cv_results = xgb.cv(params, dtrain, nfold=3, num_boost_round=1000, early_stopping_rounds=30, feval=rmspe_xg,\n",
    "                            seed=23)\n",
    "        mean_error = min(cv_results['test-rmspe-mean'])\n",
    "\n",
    "        if mean_error < min_merror:\n",
    "            min_merror = mean_error\n",
    "            xgb_best_params[\"n_estimators\"] = n_estimators\n",
    "\n",
    "    params[\"n_estimators\"] = xgb_best_params[\"n_estimators\"]\n",
    "\n",
    "    # ########################################### max_depth & min_child_weight #############################\n",
    "    min_merror = np.inf\n",
    "    for max_depth in range(6, 11, 1):\n",
    "        for min_child_weight in range(1, 6, 1):\n",
    "            params['max_depth'] = max_depth\n",
    "            params['min_child_weight'] = min_child_weight\n",
    "            cv_results = xgb.cv(params, dtrain, nfold=3, num_boost_round=1000, early_stopping_rounds=50, feval=rmspe_xg,\n",
    "                                seed=23)\n",
    "            mean_error = np.argmin(cv_results['test-rmspe-mean'])\n",
    "\n",
    "            if mean_error < min_merror:\n",
    "                min_merror = mean_error\n",
    "                xgb_best_params[\"max_depth\"] = max_depth\n",
    "                xgb_best_params[\"min_child_weight\"] = min_child_weight\n",
    "\n",
    "    params['max_depth'] = xgb_best_params['max_depth']\n",
    "    params[\"min_child_weight\"] = xgb_best_params[\"min_child_weight\"]\n",
    "\n",
    "    # ########################################### gamma #####################################################\n",
    "    for gamma in [i / 10.0 for i in range(0, 1)]:\n",
    "        params['gamma'] = gamma\n",
    "        cv_results = xgb.cv(params, dtrain, nfold=3, early_stopping_rounds=50, feval=rmspe_xg, seed=23)\n",
    "        mean_error = min(cv_results['test-rmspe-mean'])\n",
    "\n",
    "        if mean_error < min_merror:\n",
    "            min_merror = mean_error\n",
    "            xgb_best_params[\"gamma\"] = gamma\n",
    "\n",
    "    params[\"gamma\"] = xgb_best_params[\"gamma\"]\n",
    "\n",
    "    # ############################################# subsample & colsample_bytree ############################\n",
    "    min_merror = np.inf\n",
    "    for subsample in [i / 10.0 for i in range(6, 10)]:\n",
    "        for colsample_bytree in [i / 10.0 for i in range(6, 10)]:\n",
    "            params['subsample'] = subsample\n",
    "            params['colsample_bytree'] = colsample_bytree\n",
    "            cv_results = xgb.cv(params, dtrain, nfold=3, early_stopping_rounds=50, feval=rmspe_xg, seed=23)\n",
    "            mean_error = min(cv_results['test-rmspe-mean'])\n",
    "\n",
    "            if mean_error < min_merror:\n",
    "                min_merror = mean_error\n",
    "                xgb_best_params[\"subsample\"] = subsample\n",
    "                xgb_best_params[\"colsample_bytree\"] = colsample_bytree\n",
    "\n",
    "    params[\"subsample\"] = xgb_best_params[\"subsample\"]\n",
    "    params[\"colsample_bytree\"] = xgb_best_params[\"colsample_bytree\"]\n",
    "\n",
    "    # ############################################# reg_alpha ################################################\n",
    "    min_merror = np.inf\n",
    "    for reg_alpha in [0.8, 0.9, 1, 1.1, 1.2]:\n",
    "        params['reg_alpha'] = reg_alpha\n",
    "        cv_results = xgb.cv(params, dtrain, nfold=3, early_stopping_rounds=50, feval=rmspe_xg, seed=23)\n",
    "        mean_error = min(cv_results['test-rmspe-mean'])\n",
    "\n",
    "        if mean_error < min_merror:\n",
    "            min_merror = mean_error\n",
    "            xgb_best_params[\"reg_alpha\"] = reg_alpha\n",
    "\n",
    "    params[\"reg_alpha\"] = xgb_best_params[\"reg_alpha\"]\n",
    "\n",
    "    # ############################################# reg_lambda ################################################\n",
    "    min_merror = np.inf\n",
    "    for reg_lambda in [0.8, 0.9, 1, 1.1, 1.2]:\n",
    "        params['reg_lambda'] = reg_lambda\n",
    "        cv_results = xgb.cv(params, dtrain, nfold=3, early_stopping_rounds=50, feval=rmspe_xg, seed=23)\n",
    "        mean_error = min(cv_results['test-rmspe-mean'])\n",
    "\n",
    "        if mean_error < min_merror:\n",
    "            min_merror = mean_error\n",
    "            xgb_best_params[\"reg_lambda\"] = reg_lambda\n",
    "\n",
    "    params[\"reg_lambda\"] = xgb_best_params[\"reg_lambda\"]\n",
    "\n",
    "    # ############################################# learning_rate ################################################\n",
    "    min_merror = np.inf\n",
    "    for learning_rate in [0.001, 0.005, 0.01, 0.03, 0.05]:\n",
    "        params['learning_rate'] = learning_rate\n",
    "        cv_results = xgb.cv(params, dtrain, nfold=3, early_stopping_rounds=50, feval=rmspe_xg, seed=23)\n",
    "        mean_error = min(cv_results['test-rmspe-mean'])\n",
    "\n",
    "        if mean_error < min_merror:\n",
    "            min_merror = mean_error\n",
    "            xgb_best_params[\"learning_rate\"] = learning_rate\n",
    "\n",
    "    params[\"learning_rate\"] = xgb_best_params[\"learning_rate\"]\n",
    "\n",
    "    bst_params = {\"objective\": params['objective'],\n",
    "                  \"booster\": params['booster'],\n",
    "                  \"eta\": 0.3,\n",
    "                  \"max_depth\": params['max_depth'],\n",
    "                  'min_child_weight': params['min_child_weight'],\n",
    "                  \"subsample\": params['subsample'],\n",
    "                  \"colsample_bytree\": params['colsample_bytree'],\n",
    "                  \"reg_alpha\": params['reg_alpha'],\n",
    "                  \"silent\": 1,\n",
    "                  \"seed\": 2019,\n",
    "                  \"gpu_id\": params['gpu_id'],\n",
    "                  \"max_bin\": params['max_bin'],\n",
    "                  \"tree_method\": params['tree_method']\n",
    "                  }\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "    xgb_model = xgb.train(bst_params, dtrain, num_boost_round=1000, evals=watchlist, early_stopping_rounds=100,\n",
    "                          feval=rmspe_xg, verbose_eval=True)\n",
    "    print(\"Validating\")\n",
    "    yhat = xgb_model.predict(xgb.DMatrix(X_valid))\n",
    "    error = rmspe(np.expm1(y_valid.values), np.expm1(yhat))\n",
    "    print('RMSPE: {:.6f}'.format(error))\n",
    "\n",
    "xgb_model(X_train, X_valid, y_train,y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/wjunneng/Python/anaconda3/envs/lightgbm/lib/python3.6/site-packages/numpy/core/fromnumeric.py:56: FutureWarning: \nThe current behaviour of 'Series.argmin' is deprecated, use 'idxmin'\ninstead.\nThe behavior of 'argmin' will be corrected to return the positional\nminimum in the future. For now, use 'series.values.argmin' or\n'np.argmin(np.array(values))' to get the position of the minimum\nrow.\n  return getattr(obj, method)(*args, **kwds)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "[0]\ttrain-rmse:3.88865\teval-rmse:3.89556\ttrain-rmspe:0.969051\teval-rmspe:0.968478\n",
      "Multiple eval metrics have been passed: 'eval-rmspe' will be used for early stopping.\n\n",
      "Will train until eval-rmspe hasn't improved in 100 rounds.\n",
      "[1]\ttrain-rmse:2.78492\teval-rmse:2.7916\ttrain-rmspe:0.907832\teval-rmspe:0.906479\n",
      "[2]\ttrain-rmse:2.02611\teval-rmse:2.03426\ttrain-rmspe:0.829419\teval-rmspe:0.828267\n",
      "[3]\ttrain-rmse:1.51617\teval-rmse:1.52988\ttrain-rmspe:0.780363\teval-rmspe:0.789346\n",
      "[4]\ttrain-rmse:1.15867\teval-rmse:1.17336\ttrain-rmspe:0.756433\teval-rmspe:0.732812\n",
      "[5]\ttrain-rmse:0.920941\teval-rmse:0.942021\ttrain-rmspe:0.718336\teval-rmspe:0.720333\n",
      "[6]\ttrain-rmse:0.754301\teval-rmse:0.777749\ttrain-rmspe:0.665444\teval-rmspe:0.711323\n",
      "[7]\ttrain-rmse:0.659594\teval-rmse:0.683499\ttrain-rmspe:0.676871\teval-rmspe:0.742981\n",
      "[8]\ttrain-rmse:0.598845\teval-rmse:0.627509\ttrain-rmspe:0.695459\teval-rmspe:0.745061\n",
      "[9]\ttrain-rmse:0.550993\teval-rmse:0.57989\ttrain-rmspe:0.697786\teval-rmspe:0.745578\n",
      "[10]\ttrain-rmse:0.525913\teval-rmse:0.556562\ttrain-rmspe:0.703374\teval-rmspe:0.751642\n",
      "[11]\ttrain-rmse:0.481353\teval-rmse:0.513683\ttrain-rmspe:0.62107\teval-rmspe:0.693635\n",
      "[12]\ttrain-rmse:0.469337\teval-rmse:0.502988\ttrain-rmspe:0.619572\teval-rmspe:0.682768\n",
      "[13]\ttrain-rmse:0.455198\teval-rmse:0.490672\ttrain-rmspe:0.613122\teval-rmspe:0.683025\n",
      "[14]\ttrain-rmse:0.433032\teval-rmse:0.469346\ttrain-rmspe:0.583941\teval-rmspe:0.641107\n",
      "[15]\ttrain-rmse:0.415134\teval-rmse:0.452498\ttrain-rmspe:0.561418\teval-rmspe:0.614405\n",
      "[16]\ttrain-rmse:0.403669\teval-rmse:0.442406\ttrain-rmspe:0.546046\teval-rmspe:0.605384\n",
      "[17]\ttrain-rmse:0.392989\teval-rmse:0.432419\ttrain-rmspe:0.5327\teval-rmspe:0.591029\n",
      "[18]\ttrain-rmse:0.383589\teval-rmse:0.423112\ttrain-rmspe:0.518884\teval-rmspe:0.571141\n",
      "[19]\ttrain-rmse:0.371372\teval-rmse:0.41041\ttrain-rmspe:0.502781\teval-rmspe:0.556512\n",
      "[20]\ttrain-rmse:0.365712\teval-rmse:0.406586\ttrain-rmspe:0.494048\teval-rmspe:0.549666\n",
      "[21]\ttrain-rmse:0.357616\teval-rmse:0.3987\ttrain-rmspe:0.480866\teval-rmspe:0.538886\n",
      "[22]\ttrain-rmse:0.352075\teval-rmse:0.394448\ttrain-rmspe:0.472669\teval-rmspe:0.531979\n",
      "[23]\ttrain-rmse:0.346117\teval-rmse:0.388041\ttrain-rmspe:0.465405\teval-rmspe:0.525061\n",
      "[24]\ttrain-rmse:0.339615\teval-rmse:0.382188\ttrain-rmspe:0.458955\teval-rmspe:0.519912\n",
      "[25]\ttrain-rmse:0.332922\teval-rmse:0.375102\ttrain-rmspe:0.448286\teval-rmspe:0.499714\n",
      "[26]\ttrain-rmse:0.32057\teval-rmse:0.362621\ttrain-rmspe:0.42907\teval-rmspe:0.471835\n",
      "[27]\ttrain-rmse:0.318136\teval-rmse:0.36181\ttrain-rmspe:0.426031\teval-rmspe:0.471257\n",
      "[28]\ttrain-rmse:0.315213\teval-rmse:0.358883\ttrain-rmspe:0.42131\teval-rmspe:0.463876\n",
      "[29]\ttrain-rmse:0.312806\teval-rmse:0.358177\ttrain-rmspe:0.418009\teval-rmspe:0.463173\n",
      "[30]\ttrain-rmse:0.301263\teval-rmse:0.348004\ttrain-rmspe:0.380612\teval-rmspe:0.441669\n",
      "[31]\ttrain-rmse:0.296621\teval-rmse:0.342433\ttrain-rmspe:0.374516\teval-rmspe:0.430274\n",
      "[32]\ttrain-rmse:0.290924\teval-rmse:0.336045\ttrain-rmspe:0.361263\teval-rmspe:0.417712\n",
      "[33]\ttrain-rmse:0.288614\teval-rmse:0.334849\ttrain-rmspe:0.357983\teval-rmspe:0.416617\n",
      "[34]\ttrain-rmse:0.28429\teval-rmse:0.330416\ttrain-rmspe:0.351605\teval-rmspe:0.404207\n",
      "[35]\ttrain-rmse:0.277256\teval-rmse:0.323749\ttrain-rmspe:0.332765\teval-rmspe:0.391473\n",
      "[36]\ttrain-rmse:0.276087\teval-rmse:0.323362\ttrain-rmspe:0.330578\teval-rmspe:0.388453\n",
      "[37]\ttrain-rmse:0.271234\teval-rmse:0.318823\ttrain-rmspe:0.319723\teval-rmspe:0.381411\n",
      "[38]\ttrain-rmse:0.267987\teval-rmse:0.315547\ttrain-rmspe:0.315235\teval-rmspe:0.376142\n",
      "[39]\ttrain-rmse:0.263273\teval-rmse:0.310368\ttrain-rmspe:0.30749\teval-rmspe:0.369115\n",
      "[40]\ttrain-rmse:0.260717\teval-rmse:0.307937\ttrain-rmspe:0.302934\teval-rmspe:0.366118\n",
      "[41]\ttrain-rmse:0.258643\teval-rmse:0.306441\ttrain-rmspe:0.299978\teval-rmspe:0.364717\n",
      "[42]\ttrain-rmse:0.25578\teval-rmse:0.303331\ttrain-rmspe:0.295751\teval-rmspe:0.360323\n",
      "[43]\ttrain-rmse:0.253871\teval-rmse:0.302352\ttrain-rmspe:0.292272\teval-rmspe:0.359671\n",
      "[44]\ttrain-rmse:0.251346\teval-rmse:0.30022\ttrain-rmspe:0.288768\teval-rmspe:0.357652\n",
      "[45]\ttrain-rmse:0.249423\teval-rmse:0.298801\ttrain-rmspe:0.286198\teval-rmspe:0.356144\n",
      "[46]\ttrain-rmse:0.243193\teval-rmse:0.29313\ttrain-rmspe:0.278155\teval-rmspe:0.349053\n",
      "[47]\ttrain-rmse:0.240598\teval-rmse:0.290581\ttrain-rmspe:0.27355\teval-rmspe:0.345115\n",
      "[48]\ttrain-rmse:0.237646\teval-rmse:0.287607\ttrain-rmspe:0.267809\teval-rmspe:0.339104\n",
      "[49]\ttrain-rmse:0.234194\teval-rmse:0.284508\ttrain-rmspe:0.263301\teval-rmspe:0.334842\n",
      "[50]\ttrain-rmse:0.231232\teval-rmse:0.282113\ttrain-rmspe:0.258834\teval-rmspe:0.331775\n",
      "[51]\ttrain-rmse:0.22897\teval-rmse:0.28041\ttrain-rmspe:0.255781\teval-rmspe:0.330212\n",
      "[52]\ttrain-rmse:0.228178\teval-rmse:0.280024\ttrain-rmspe:0.254803\teval-rmspe:0.329518\n",
      "[53]\ttrain-rmse:0.226495\teval-rmse:0.278239\ttrain-rmspe:0.252674\teval-rmspe:0.325871\n",
      "[54]\ttrain-rmse:0.223308\teval-rmse:0.275327\ttrain-rmspe:0.248929\teval-rmspe:0.322874\n",
      "[55]\ttrain-rmse:0.22021\teval-rmse:0.272888\ttrain-rmspe:0.245213\teval-rmspe:0.320232\n",
      "[56]\ttrain-rmse:0.217495\teval-rmse:0.270445\ttrain-rmspe:0.241277\teval-rmspe:0.31706\n",
      "[57]\ttrain-rmse:0.216448\teval-rmse:0.269471\ttrain-rmspe:0.239861\teval-rmspe:0.315865\n",
      "[58]\ttrain-rmse:0.214623\teval-rmse:0.267799\ttrain-rmspe:0.237532\teval-rmspe:0.314298\n",
      "[59]\ttrain-rmse:0.213729\teval-rmse:0.26706\ttrain-rmspe:0.236356\teval-rmspe:0.312567\n",
      "[60]\ttrain-rmse:0.2133\teval-rmse:0.266893\ttrain-rmspe:0.235865\teval-rmspe:0.312244\n",
      "[61]\ttrain-rmse:0.210317\teval-rmse:0.263397\ttrain-rmspe:0.232295\teval-rmspe:0.305306\n",
      "[62]\ttrain-rmse:0.207633\teval-rmse:0.261074\ttrain-rmspe:0.228473\teval-rmspe:0.302785\n",
      "[63]\ttrain-rmse:0.206026\teval-rmse:0.259507\ttrain-rmspe:0.226516\teval-rmspe:0.300909\n",
      "[64]\ttrain-rmse:0.201943\teval-rmse:0.255668\ttrain-rmspe:0.222266\teval-rmspe:0.297062\n",
      "[65]\ttrain-rmse:0.199063\teval-rmse:0.252686\ttrain-rmspe:0.219003\teval-rmspe:0.293797\n",
      "[66]\ttrain-rmse:0.197866\teval-rmse:0.251704\ttrain-rmspe:0.217687\teval-rmspe:0.292845\n",
      "[67]\ttrain-rmse:0.196372\teval-rmse:0.250048\ttrain-rmspe:0.21581\teval-rmspe:0.289522\n",
      "[68]\ttrain-rmse:0.194414\teval-rmse:0.248102\ttrain-rmspe:0.213362\teval-rmspe:0.287939\n",
      "[69]\ttrain-rmse:0.192979\teval-rmse:0.247133\ttrain-rmspe:0.21163\teval-rmspe:0.286936\n",
      "[70]\ttrain-rmse:0.191709\teval-rmse:0.246069\ttrain-rmspe:0.209853\teval-rmspe:0.285669\n",
      "[71]\ttrain-rmse:0.190873\teval-rmse:0.245536\ttrain-rmspe:0.208815\teval-rmspe:0.28513\n",
      "[72]\ttrain-rmse:0.190071\teval-rmse:0.244726\ttrain-rmspe:0.207767\teval-rmspe:0.283985\n",
      "[73]\ttrain-rmse:0.188224\teval-rmse:0.243182\ttrain-rmspe:0.205514\teval-rmspe:0.282259\n",
      "[74]\ttrain-rmse:0.186983\teval-rmse:0.241919\ttrain-rmspe:0.20398\teval-rmspe:0.280595\n",
      "[75]\ttrain-rmse:0.185861\teval-rmse:0.240875\ttrain-rmspe:0.202176\teval-rmspe:0.279379\n",
      "[76]\ttrain-rmse:0.184908\teval-rmse:0.240052\ttrain-rmspe:0.200693\teval-rmspe:0.278385\n",
      "[77]\ttrain-rmse:0.183369\teval-rmse:0.238172\ttrain-rmspe:0.198822\teval-rmspe:0.276496\n",
      "[78]\ttrain-rmse:0.182418\teval-rmse:0.237713\ttrain-rmspe:0.197761\teval-rmspe:0.276164\n",
      "[79]\ttrain-rmse:0.181595\teval-rmse:0.237007\ttrain-rmspe:0.196841\teval-rmspe:0.275214\n",
      "[80]\ttrain-rmse:0.180758\teval-rmse:0.236217\ttrain-rmspe:0.195802\teval-rmspe:0.274288\n",
      "[81]\ttrain-rmse:0.179242\teval-rmse:0.23509\ttrain-rmspe:0.19349\teval-rmspe:0.272496\n",
      "[82]\ttrain-rmse:0.1782\teval-rmse:0.234315\ttrain-rmspe:0.192317\teval-rmspe:0.271574\n",
      "[83]\ttrain-rmse:0.177237\teval-rmse:0.233337\ttrain-rmspe:0.191132\teval-rmspe:0.270417\n",
      "[84]\ttrain-rmse:0.17672\teval-rmse:0.232911\ttrain-rmspe:0.190186\teval-rmspe:0.269851\n",
      "[85]\ttrain-rmse:0.176044\teval-rmse:0.232192\ttrain-rmspe:0.189399\teval-rmspe:0.26865\n",
      "[86]\ttrain-rmse:0.174224\teval-rmse:0.23037\ttrain-rmspe:0.187125\teval-rmspe:0.266468\n",
      "[87]\ttrain-rmse:0.173322\teval-rmse:0.229838\ttrain-rmspe:0.186106\teval-rmspe:0.266871\n",
      "[88]\ttrain-rmse:0.171923\teval-rmse:0.2284\ttrain-rmspe:0.18428\teval-rmspe:0.265266\n",
      "[89]\ttrain-rmse:0.170286\teval-rmse:0.226565\ttrain-rmspe:0.182518\teval-rmspe:0.263521\n",
      "[90]\ttrain-rmse:0.168671\teval-rmse:0.224932\ttrain-rmspe:0.180696\teval-rmspe:0.261554\n",
      "[91]\ttrain-rmse:0.16798\teval-rmse:0.22422\ttrain-rmspe:0.179971\teval-rmspe:0.261201\n",
      "[92]\ttrain-rmse:0.167268\teval-rmse:0.223662\ttrain-rmspe:0.179078\teval-rmspe:0.260597\n",
      "[93]\ttrain-rmse:0.166524\teval-rmse:0.222831\ttrain-rmspe:0.178284\teval-rmspe:0.259814\n",
      "[94]\ttrain-rmse:0.165989\teval-rmse:0.222314\ttrain-rmspe:0.17773\teval-rmspe:0.25948\n",
      "[95]\ttrain-rmse:0.165601\teval-rmse:0.222142\ttrain-rmspe:0.177232\teval-rmspe:0.259283\n",
      "[96]\ttrain-rmse:0.164941\teval-rmse:0.221542\ttrain-rmspe:0.176434\teval-rmspe:0.258324\n",
      "[97]\ttrain-rmse:0.164132\teval-rmse:0.220681\ttrain-rmspe:0.175515\teval-rmspe:0.257373\n",
      "[98]\ttrain-rmse:0.163571\teval-rmse:0.220202\ttrain-rmspe:0.174639\teval-rmspe:0.256844\n",
      "[99]\ttrain-rmse:0.162804\teval-rmse:0.219383\ttrain-rmspe:0.173605\teval-rmspe:0.255951\n",
      "[100]\ttrain-rmse:0.162208\teval-rmse:0.218725\ttrain-rmspe:0.17295\teval-rmspe:0.255257\n",
      "[101]\ttrain-rmse:0.161461\teval-rmse:0.218147\ttrain-rmspe:0.172083\teval-rmspe:0.25618\n",
      "[102]\ttrain-rmse:0.161179\teval-rmse:0.217872\ttrain-rmspe:0.171778\teval-rmspe:0.255806\n",
      "[103]\ttrain-rmse:0.16073\teval-rmse:0.217727\ttrain-rmspe:0.171126\teval-rmspe:0.255591\n",
      "[104]\ttrain-rmse:0.160175\teval-rmse:0.217479\ttrain-rmspe:0.170344\teval-rmspe:0.255144\n",
      "[105]\ttrain-rmse:0.159683\teval-rmse:0.217283\ttrain-rmspe:0.169572\teval-rmspe:0.25479\n",
      "[106]\ttrain-rmse:0.159256\teval-rmse:0.216992\ttrain-rmspe:0.169058\teval-rmspe:0.25395\n",
      "[107]\ttrain-rmse:0.158645\teval-rmse:0.216426\ttrain-rmspe:0.168397\teval-rmspe:0.253161\n",
      "[108]\ttrain-rmse:0.158195\teval-rmse:0.216004\ttrain-rmspe:0.167785\teval-rmspe:0.252082\n",
      "[109]\ttrain-rmse:0.157767\teval-rmse:0.215541\ttrain-rmspe:0.167343\teval-rmspe:0.251481\n",
      "[110]\ttrain-rmse:0.157275\teval-rmse:0.215291\ttrain-rmspe:0.166649\teval-rmspe:0.251963\n",
      "[111]\ttrain-rmse:0.156508\teval-rmse:0.214627\ttrain-rmspe:0.165501\teval-rmspe:0.252849\n",
      "[112]\ttrain-rmse:0.155725\teval-rmse:0.214052\ttrain-rmspe:0.164599\teval-rmspe:0.25068\n",
      "[113]\ttrain-rmse:0.15541\teval-rmse:0.213789\ttrain-rmspe:0.164268\teval-rmspe:0.250504\n",
      "[114]\ttrain-rmse:0.154806\teval-rmse:0.213325\ttrain-rmspe:0.163597\teval-rmspe:0.249993\n",
      "[115]\ttrain-rmse:0.154173\teval-rmse:0.212825\ttrain-rmspe:0.162817\teval-rmspe:0.249576\n",
      "[116]\ttrain-rmse:0.153817\teval-rmse:0.21228\ttrain-rmspe:0.162382\teval-rmspe:0.248784\n",
      "[117]\ttrain-rmse:0.153394\teval-rmse:0.211971\ttrain-rmspe:0.161908\teval-rmspe:0.248472\n",
      "[118]\ttrain-rmse:0.152738\teval-rmse:0.211152\ttrain-rmspe:0.161197\teval-rmspe:0.247802\n",
      "[119]\ttrain-rmse:0.152371\teval-rmse:0.210893\ttrain-rmspe:0.160764\teval-rmspe:0.247546\n",
      "[120]\ttrain-rmse:0.151978\teval-rmse:0.21049\ttrain-rmspe:0.160265\teval-rmspe:0.247115\n",
      "[121]\ttrain-rmse:0.151591\teval-rmse:0.210269\ttrain-rmspe:0.159735\teval-rmspe:0.246507\n",
      "[122]\ttrain-rmse:0.15105\teval-rmse:0.209881\ttrain-rmspe:0.159177\teval-rmspe:0.246084\n",
      "[123]\ttrain-rmse:0.150662\teval-rmse:0.209453\ttrain-rmspe:0.158742\teval-rmspe:0.245643\n",
      "[124]\ttrain-rmse:0.150151\teval-rmse:0.20906\ttrain-rmspe:0.158149\teval-rmspe:0.245213\n",
      "[125]\ttrain-rmse:0.149643\teval-rmse:0.208686\ttrain-rmspe:0.157568\teval-rmspe:0.245463\n",
      "[126]\ttrain-rmse:0.149013\teval-rmse:0.208218\ttrain-rmspe:0.156628\teval-rmspe:0.2449\n",
      "[127]\ttrain-rmse:0.148818\teval-rmse:0.208086\ttrain-rmspe:0.15632\teval-rmspe:0.245131\n",
      "[128]\ttrain-rmse:0.148539\teval-rmse:0.208077\ttrain-rmspe:0.156016\teval-rmspe:0.246784\n",
      "[129]\ttrain-rmse:0.148233\teval-rmse:0.207866\ttrain-rmspe:0.155689\teval-rmspe:0.246553\n",
      "[130]\ttrain-rmse:0.147865\teval-rmse:0.20751\ttrain-rmspe:0.155154\teval-rmspe:0.246143\n",
      "[131]\ttrain-rmse:0.147578\teval-rmse:0.20722\ttrain-rmspe:0.154806\teval-rmspe:0.245848\n",
      "[132]\ttrain-rmse:0.147194\teval-rmse:0.207034\ttrain-rmspe:0.154366\teval-rmspe:0.245726\n",
      "[133]\ttrain-rmse:0.146721\teval-rmse:0.206644\ttrain-rmspe:0.153834\teval-rmspe:0.245255\n",
      "[134]\ttrain-rmse:0.14616\teval-rmse:0.206067\ttrain-rmspe:0.153177\teval-rmspe:0.24468\n",
      "[135]\ttrain-rmse:0.145704\teval-rmse:0.20561\ttrain-rmspe:0.152664\teval-rmspe:0.244807\n",
      "[136]\ttrain-rmse:0.145462\teval-rmse:0.205486\ttrain-rmspe:0.152411\teval-rmspe:0.2447\n",
      "[137]\ttrain-rmse:0.145125\teval-rmse:0.205223\ttrain-rmspe:0.151971\teval-rmspe:0.24444\n",
      "[138]\ttrain-rmse:0.144772\teval-rmse:0.204977\ttrain-rmspe:0.151626\teval-rmspe:0.244264\n",
      "[139]\ttrain-rmse:0.144427\teval-rmse:0.204791\ttrain-rmspe:0.15111\teval-rmspe:0.244094\n",
      "[140]\ttrain-rmse:0.144226\teval-rmse:0.204651\ttrain-rmspe:0.150893\teval-rmspe:0.243884\n",
      "[141]\ttrain-rmse:0.144001\teval-rmse:0.20448\ttrain-rmspe:0.150672\teval-rmspe:0.243788\n",
      "[142]\ttrain-rmse:0.143628\teval-rmse:0.204141\ttrain-rmspe:0.150218\teval-rmspe:0.243726\n",
      "[143]\ttrain-rmse:0.143384\teval-rmse:0.203968\ttrain-rmspe:0.149956\teval-rmspe:0.243585\n",
      "[144]\ttrain-rmse:0.143131\teval-rmse:0.203659\ttrain-rmspe:0.149678\teval-rmspe:0.243215\n",
      "[145]\ttrain-rmse:0.142874\teval-rmse:0.203356\ttrain-rmspe:0.149364\teval-rmspe:0.242717\n",
      "[146]\ttrain-rmse:0.14273\teval-rmse:0.203291\ttrain-rmspe:0.149212\teval-rmspe:0.242672\n",
      "[147]\ttrain-rmse:0.142316\teval-rmse:0.202938\ttrain-rmspe:0.148612\teval-rmspe:0.242278\n",
      "[148]\ttrain-rmse:0.142125\teval-rmse:0.202795\ttrain-rmspe:0.148367\teval-rmspe:0.242114\n",
      "[149]\ttrain-rmse:0.141841\teval-rmse:0.202511\ttrain-rmspe:0.147966\teval-rmspe:0.241923\n",
      "[150]\ttrain-rmse:0.141693\teval-rmse:0.202418\ttrain-rmspe:0.147795\teval-rmspe:0.2418\n",
      "[151]\ttrain-rmse:0.141563\teval-rmse:0.202239\ttrain-rmspe:0.147645\teval-rmspe:0.241495\n",
      "[152]\ttrain-rmse:0.141387\teval-rmse:0.202125\ttrain-rmspe:0.147479\teval-rmspe:0.240355\n",
      "[153]\ttrain-rmse:0.141067\teval-rmse:0.201851\ttrain-rmspe:0.147139\teval-rmspe:0.240093\n",
      "[154]\ttrain-rmse:0.140858\teval-rmse:0.201775\ttrain-rmspe:0.146884\teval-rmspe:0.240188\n",
      "[155]\ttrain-rmse:0.140628\teval-rmse:0.201596\ttrain-rmspe:0.146656\teval-rmspe:0.240082\n",
      "[156]\ttrain-rmse:0.140382\teval-rmse:0.201307\ttrain-rmspe:0.146395\teval-rmspe:0.239783\n",
      "[157]\ttrain-rmse:0.140099\teval-rmse:0.201224\ttrain-rmspe:0.146\teval-rmspe:0.239702\n",
      "[158]\ttrain-rmse:0.139985\teval-rmse:0.201127\ttrain-rmspe:0.145871\teval-rmspe:0.239435\n",
      "[159]\ttrain-rmse:0.139818\teval-rmse:0.200972\ttrain-rmspe:0.145694\teval-rmspe:0.239288\n",
      "[160]\ttrain-rmse:0.139587\teval-rmse:0.200809\ttrain-rmspe:0.145457\teval-rmspe:0.239226\n",
      "[161]\ttrain-rmse:0.139369\teval-rmse:0.200535\ttrain-rmspe:0.14522\teval-rmspe:0.238895\n",
      "[162]\ttrain-rmse:0.139228\teval-rmse:0.200443\ttrain-rmspe:0.145103\teval-rmspe:0.238868\n",
      "[163]\ttrain-rmse:0.139051\teval-rmse:0.200312\ttrain-rmspe:0.144921\teval-rmspe:0.238771\n",
      "[164]\ttrain-rmse:0.138812\teval-rmse:0.200177\ttrain-rmspe:0.144634\teval-rmspe:0.238668\n",
      "[165]\ttrain-rmse:0.138644\teval-rmse:0.200057\ttrain-rmspe:0.144429\teval-rmspe:0.23836\n",
      "[166]\ttrain-rmse:0.138431\teval-rmse:0.19994\ttrain-rmspe:0.144167\teval-rmspe:0.238046\n",
      "[167]\ttrain-rmse:0.13829\teval-rmse:0.19984\ttrain-rmspe:0.144022\teval-rmspe:0.237991\n",
      "[168]\ttrain-rmse:0.138145\teval-rmse:0.199818\ttrain-rmspe:0.143843\teval-rmspe:0.238003\n",
      "[169]\ttrain-rmse:0.138034\teval-rmse:0.199735\ttrain-rmspe:0.143705\teval-rmspe:0.237849\n",
      "[170]\ttrain-rmse:0.137896\teval-rmse:0.19974\ttrain-rmspe:0.143554\teval-rmspe:0.237593\n",
      "[171]\ttrain-rmse:0.137775\teval-rmse:0.19961\ttrain-rmspe:0.14343\teval-rmspe:0.237467\n",
      "[172]\ttrain-rmse:0.137665\teval-rmse:0.199567\ttrain-rmspe:0.14331\teval-rmspe:0.237414\n",
      "[173]\ttrain-rmse:0.137519\teval-rmse:0.199392\ttrain-rmspe:0.143146\teval-rmspe:0.23725\n",
      "[174]\ttrain-rmse:0.137374\teval-rmse:0.199282\ttrain-rmspe:0.142981\teval-rmspe:0.237133\n",
      "[175]\ttrain-rmse:0.137301\teval-rmse:0.199201\ttrain-rmspe:0.142903\teval-rmspe:0.23705\n",
      "[176]\ttrain-rmse:0.137151\teval-rmse:0.199097\ttrain-rmspe:0.142749\teval-rmspe:0.237554\n",
      "[177]\ttrain-rmse:0.136985\teval-rmse:0.199032\ttrain-rmspe:0.142564\teval-rmspe:0.237504\n",
      "[178]\ttrain-rmse:0.136794\teval-rmse:0.198886\ttrain-rmspe:0.142364\teval-rmspe:0.237498\n",
      "[179]\ttrain-rmse:0.136578\teval-rmse:0.198814\ttrain-rmspe:0.142164\teval-rmspe:0.237428\n",
      "[180]\ttrain-rmse:0.136495\teval-rmse:0.198752\ttrain-rmspe:0.142077\teval-rmspe:0.237352\n",
      "[181]\ttrain-rmse:0.136332\teval-rmse:0.19867\ttrain-rmspe:0.141902\teval-rmspe:0.237238\n",
      "[182]\ttrain-rmse:0.136208\teval-rmse:0.198559\ttrain-rmspe:0.141765\teval-rmspe:0.237001\n",
      "[183]\ttrain-rmse:0.135995\teval-rmse:0.198334\ttrain-rmspe:0.141524\teval-rmspe:0.237378\n",
      "[184]\ttrain-rmse:0.135816\teval-rmse:0.198082\ttrain-rmspe:0.141313\teval-rmspe:0.236854\n",
      "[185]\ttrain-rmse:0.135715\teval-rmse:0.197995\ttrain-rmspe:0.141187\teval-rmspe:0.236531\n",
      "[186]\ttrain-rmse:0.135605\teval-rmse:0.197927\ttrain-rmspe:0.141087\teval-rmspe:0.236463\n",
      "[187]\ttrain-rmse:0.135458\teval-rmse:0.197815\ttrain-rmspe:0.140934\teval-rmspe:0.236353\n",
      "[188]\ttrain-rmse:0.135293\teval-rmse:0.197734\ttrain-rmspe:0.14069\teval-rmspe:0.236299\n",
      "[189]\ttrain-rmse:0.135164\teval-rmse:0.197669\ttrain-rmspe:0.140469\teval-rmspe:0.236237\n",
      "[190]\ttrain-rmse:0.135023\teval-rmse:0.19756\ttrain-rmspe:0.140326\teval-rmspe:0.236162\n",
      "[191]\ttrain-rmse:0.134874\teval-rmse:0.197509\ttrain-rmspe:0.140139\teval-rmspe:0.236138\n",
      "[192]\ttrain-rmse:0.134727\teval-rmse:0.197393\ttrain-rmspe:0.139894\teval-rmspe:0.236474\n",
      "[193]\ttrain-rmse:0.134603\teval-rmse:0.197269\ttrain-rmspe:0.139759\teval-rmspe:0.23641\n",
      "[194]\ttrain-rmse:0.134434\teval-rmse:0.197131\ttrain-rmspe:0.139565\teval-rmspe:0.236298\n",
      "[195]\ttrain-rmse:0.13428\teval-rmse:0.196982\ttrain-rmspe:0.139398\teval-rmspe:0.236146\n",
      "[196]\ttrain-rmse:0.134147\teval-rmse:0.196875\ttrain-rmspe:0.139236\teval-rmspe:0.236087\n",
      "[197]\ttrain-rmse:0.133963\teval-rmse:0.196758\ttrain-rmspe:0.139043\teval-rmspe:0.235961\n",
      "[198]\ttrain-rmse:0.133869\teval-rmse:0.196672\ttrain-rmspe:0.138928\teval-rmspe:0.235902\n",
      "[199]\ttrain-rmse:0.13371\teval-rmse:0.19651\ttrain-rmspe:0.138758\teval-rmspe:0.235749\n",
      "[200]\ttrain-rmse:0.133584\teval-rmse:0.196445\ttrain-rmspe:0.138596\teval-rmspe:0.235352\n",
      "[201]\ttrain-rmse:0.133496\teval-rmse:0.196369\ttrain-rmspe:0.138495\teval-rmspe:0.235277\n",
      "[202]\ttrain-rmse:0.133314\teval-rmse:0.196238\ttrain-rmspe:0.138295\teval-rmspe:0.235047\n",
      "[203]\ttrain-rmse:0.133158\teval-rmse:0.196131\ttrain-rmspe:0.138131\teval-rmspe:0.234957\n",
      "[204]\ttrain-rmse:0.133044\teval-rmse:0.196\ttrain-rmspe:0.138006\teval-rmspe:0.234823\n",
      "[205]\ttrain-rmse:0.132921\teval-rmse:0.195947\ttrain-rmspe:0.137888\teval-rmspe:0.234333\n",
      "[206]\ttrain-rmse:0.132822\teval-rmse:0.195927\ttrain-rmspe:0.137773\teval-rmspe:0.234147\n",
      "[207]\ttrain-rmse:0.13271\teval-rmse:0.19586\ttrain-rmspe:0.137638\teval-rmspe:0.234077\n",
      "[208]\ttrain-rmse:0.132609\teval-rmse:0.195786\ttrain-rmspe:0.13751\teval-rmspe:0.234061\n",
      "[209]\ttrain-rmse:0.132528\teval-rmse:0.195757\ttrain-rmspe:0.137415\teval-rmspe:0.233968\n",
      "[210]\ttrain-rmse:0.132408\teval-rmse:0.195651\ttrain-rmspe:0.1373\teval-rmspe:0.233867\n",
      "[211]\ttrain-rmse:0.132304\teval-rmse:0.195595\ttrain-rmspe:0.13718\teval-rmspe:0.233829\n",
      "[212]\ttrain-rmse:0.132227\teval-rmse:0.195535\ttrain-rmspe:0.1371\teval-rmspe:0.23379\n",
      "[213]\ttrain-rmse:0.132146\teval-rmse:0.195493\ttrain-rmspe:0.137003\teval-rmspe:0.23376\n",
      "[214]\ttrain-rmse:0.132051\teval-rmse:0.195419\ttrain-rmspe:0.136905\teval-rmspe:0.233701\n",
      "[215]\ttrain-rmse:0.131968\teval-rmse:0.195323\ttrain-rmspe:0.136809\teval-rmspe:0.233349\n",
      "[216]\ttrain-rmse:0.131886\teval-rmse:0.195256\ttrain-rmspe:0.136711\teval-rmspe:0.23256\n",
      "[217]\ttrain-rmse:0.131798\teval-rmse:0.195201\ttrain-rmspe:0.136626\teval-rmspe:0.232561\n",
      "[218]\ttrain-rmse:0.131734\teval-rmse:0.195195\ttrain-rmspe:0.136564\teval-rmspe:0.232575\n",
      "[219]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[220]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[221]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[222]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[223]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[224]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[225]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[226]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[227]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[228]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[229]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[230]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[231]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[232]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[233]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[234]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[235]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[236]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[237]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[238]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[239]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[240]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[241]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[242]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[243]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[244]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[245]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[246]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[247]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[248]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[249]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[250]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[251]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[252]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[253]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[254]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[255]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[256]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[257]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[258]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[259]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[260]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[261]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[262]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[263]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[264]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[265]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[266]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[267]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[268]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[269]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[270]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[271]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[272]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[273]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[274]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[275]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[276]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[277]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[278]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[279]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[280]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[281]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[282]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[283]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[284]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[285]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[286]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[287]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[288]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[289]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[290]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[291]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[292]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[293]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[294]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[295]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[296]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[297]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[298]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[299]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[300]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[301]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[302]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[303]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[304]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[305]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[306]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[307]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[308]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[309]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[310]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[311]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[312]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[313]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[314]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[315]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[316]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[317]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[318]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "[319]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n",
      "Stopping. Best iteration:\n[219]\ttrain-rmse:0.131573\teval-rmse:0.195082\ttrain-rmspe:0.136317\teval-rmspe:0.232465\n\n",
      "Validating\nRMSPE: 0.232465\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "pycharm-e50da876",
   "language": "python",
   "display_name": "PyCharm (ForecastScore)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}